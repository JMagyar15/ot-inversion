{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make use of the analytical solution for the Wasserstein distance in 1D. By projecting components of the posterior mixture onto their principle axes, the variables in the new basis become independent and therefore can be considered separately. The maps therefore must be chosen so that their projections onto these axes are mapped to the standard normal (1D). We could potentially do this by minimising the Wasserstein distance between them. The advantage of this is that we can rescale the two distributions arbitrarily so that they both some to one as discrete distributions as the scaling is absorbed into the proportionallity factor that we only care about at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.linalg import fractional_matrix_power as fmp\n",
    "\n",
    "from Core_Functions import gmm_base as gb\n",
    "from Core_Functions import gmm_plot as gp\n",
    "\n",
    "import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.107022139229891"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = multivariate_normal(mean = 0, cov = 1)\n",
    "\n",
    "proj = multivariate_normal(mean = 2, cov = 0.5)\n",
    "\n",
    "x = np.linspace(-5,5,30)\n",
    "\n",
    "C = np.zeros([30,30])\n",
    "\n",
    "for i in range(30):\n",
    "    for j in range(30):\n",
    "        C[i,j] = (x[i] - x[j])**2\n",
    "        \n",
    "f = ref.pdf(x)\n",
    "f /= np.sum(f)\n",
    "g = proj.pdf(x)\n",
    "g /= np.sum(g)\n",
    "\n",
    "ot.emd2(f,g,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_Mixture(f,x):\n",
    "    \"\"\"\n",
    "    Gives the likelihood at any given set of points in a Guassian mixture model.\n",
    "    Inputs:\n",
    "        f: mixture being evaluated (Gaussian_Mixture)\n",
    "        x: points being evaluated at (array)\n",
    "    \"\"\"\n",
    "    num_x, d = np.shape(x)\n",
    "    \n",
    "    #do check here that d = f.d, error if not\n",
    "    fx = np.zeros([num_x])\n",
    "    \n",
    "    for i in range(f.n):\n",
    "        fx += f.w[i] * multivariate_normal.pdf(x,mean=f.m[i,:],cov=f.cov[i,:,:])\n",
    "    \n",
    "    return fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation Problem\n",
    "\n",
    "We have the goal of minimising the following expression:\n",
    "\n",
    "\\begin{equation}\n",
    "M = \\sum_{k=1}^m \\mathcal{W}_2^2 (\\phi, Q_k (\\Lambda, \\mathbf{b}))\n",
    "\\end{equation}\n",
    "\n",
    "We therefore firstly need a way to compute the distributions $Q_k$ for any given inputs, then find the derivative of the Wasserstein distance with respect to these inputs. The distributions will be defined as follows.\n",
    "\n",
    "\\begin{equation}\n",
    "Q_k(m) = \\frac{\\lambda_k}{s} L\\circ \\beta_k(m) h\\circ \\beta_k(m) C_k(m)\n",
    "\\end{equation}\n",
    "\n",
    "This function $C_k$ is the classifier and in turn is defined as follows.\n",
    "\n",
    "\\begin{equation}\n",
    "C_k(m) = \\frac{h\\circ \\beta_k^{-1}(m)}{\\sum_{j=1}^m q_j h\\circ \\beta_j^{-1}(m)}\n",
    "\\end{equation}\n",
    "\n",
    "Each of these maps $\\beta$ are defined by a scaling and shift.\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_k(m) = \\lambda_k m + b_k\n",
    "\\end{equation}\n",
    "\n",
    "There is the additional constraint on $\\lambda_k$ that they must be positive to ensure the transport is optimal (gradient of a convex function). \n",
    "\n",
    "The final goal will be to compute\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial M}{\\partial \\lambda_j}, \\frac{\\partial M}{\\partial b_j}, \\frac{\\partial M}{\\partial q_j}\n",
    "\\end{equation}\n",
    "\n",
    "[WILL ONLY NEED TO KEEP SOME OF THIS]\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C_k}{\\partial q_j} = -\\left(\\frac{h\\circ\\beta_j^{-1}(m)}{\\sum_{l=1}^m q_l h\\circ \\beta_l^{-1}(m)}\\right)^2 = -C_j(m)^2\n",
    "\\end{equation}\n",
    "\n",
    "It is important to remember here that, by nature of the maps,\n",
    "\n",
    "\\begin{equation}\n",
    "h\\circ\\beta_k^{-1}(m) = \\phi(m;b_k,\\lambda_k^2)\n",
    "\\end{equation}\n",
    "\n",
    "If $\\sigma_k^2$ is the variance of the $k$-th component, then\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\sigma_k^2}{\\partial \\lambda_k} &= 2\\lambda_k \\\\\n",
    "\\frac{\\partial \\sigma_k^2}{\\partial b_k} &= 0\n",
    "\\end{align}\n",
    "\n",
    "As for the mean $\\mu_k$,\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mu_k}{\\partial \\lambda_k} &= 0 \\\\\n",
    "\\frac{\\partial \\mu_k}{\\partial b_k} &= 1\n",
    "\\end{align}\n",
    "\n",
    "These must then be linked to the change in sample heights. [HAVE THESE IN MY BOOK SOMEWHERE]\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\phi}{\\partial q_j} &= 0\\\\\n",
    "\\frac{\\partial \\phi}{\\partial \\mu_j} &= \\frac{m - \\mu_j}{\\sigma_j}\\phi_j \\\\\n",
    "\\frac{\\partial \\phi}{\\partial \\sigma_j^2} &= \\left[ \\frac{(m-\\mu_j)^2}{\\sigma_j^3} - \\sigma_j \\right] \\phi_j\n",
    "\\end{align}\n",
    "\n",
    "We will then need the derivatives of the 2-Wasserstein distance with respect to mass changes and position shifts. [HAD BOTH OF THESE FOR ENTROPIC REGULARISATION SO SHOULD HAVE EQUIVALENT FOR ANALYTICAL SOLUTION HERE TOO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: First test for 2D is to show that the minimum summed Wasserstein along the model axes is more that when using the known principle axes. If this doesn't work, need to rethink theory...\n",
    "\n",
    "NOTE: how are we going to work out the marginal distributions from unevenly sampled posterior? Weighting by area of nearest neightbour cells would probably work but how computationally intense is this? Would surely be quicker than the posterior evaluation step, is purely based on model candidates and not the forward model - geometry problem.\n",
    "\n",
    "NOTE: 1D solution is dependent on CDFs so might be able to deal with projections without having to use radial basis functions or similar - think of it as mass of regions rather than mass are particular points. See diagram in Peyre and Cuturi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
