{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make use of the analytical solution for the Wasserstein distance in 1D. By projecting components of the posterior mixture onto their principle axes, the variables in the new basis become independent and therefore can be considered separately. The maps therefore must be chosen so that their projections onto these axes are mapped to the standard normal (1D). We could potentially do this by minimising the Wasserstein distance between them. The advantage of this is that we can rescale the two distributions arbitrarily so that they both some to one as discrete distributions as the scaling is absorbed into the proportionallity factor that we only care about at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.linalg import fractional_matrix_power as fmp\n",
    "\n",
    "from Core_Functions import gmm_base as gb\n",
    "from Core_Functions import gmm_plot as gp\n",
    "\n",
    "import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.107022139229891"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = multivariate_normal(mean = 0, cov = 1)\n",
    "\n",
    "proj = multivariate_normal(mean = 2, cov = 0.5)\n",
    "\n",
    "x = np.linspace(-5,5,30)\n",
    "\n",
    "C = np.zeros([30,30])\n",
    "\n",
    "for i in range(30):\n",
    "    for j in range(30):\n",
    "        C[i,j] = (x[i] - x[j])**2\n",
    "        \n",
    "f = ref.pdf(x)\n",
    "f /= np.sum(f)\n",
    "g = proj.pdf(x)\n",
    "g /= np.sum(g)\n",
    "\n",
    "ot.emd2(f,g,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_Mixture(f,x):\n",
    "    \"\"\"\n",
    "    Gives the likelihood at any given set of points in a Guassian mixture model.\n",
    "    Inputs:\n",
    "        f: mixture being evaluated (Gaussian_Mixture)\n",
    "        x: points being evaluated at (array)\n",
    "    \"\"\"\n",
    "    num_x, d = np.shape(x)\n",
    "    \n",
    "    #do check here that d = f.d, error if not\n",
    "    fx = np.zeros([num_x])\n",
    "    \n",
    "    for i in range(f.n):\n",
    "        fx += f.w[i] * multivariate_normal.pdf(x,mean=f.m[i,:],cov=f.cov[i,:,:])\n",
    "    \n",
    "    return fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation Problem\n",
    "\n",
    "We have the goal of minimising the following expression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\Lambda, \\mathbf{b}} \\sum_{k=1}^m \\mathcal{W}_2^2 (\\phi, Q_k (\\Lambda, \\mathbf{b}))\n",
    "\\end{equation}\n",
    "\n",
    "We therefore firstly need a way to compute the distribution $Q_k$ for any given inputs, then find the derivative of the Wasserstein distance with respect to these inputs. The distribution will be defined as follows.\n",
    "\n",
    "\\begin{equation}\n",
    "Q_k(m) = \\frac{\\lambda_k}{s} L\\circ \\beta_k(m) h\\circ \\beta_k(m) C_k(m)\n",
    "\\end{equation}\n",
    "\n",
    "This function $C_k$ is the classifier and in turn is defined as follows.\n",
    "\n",
    "\\begin{equation}\n",
    "C_k(m) = \\frac{h\\circ \\beta_k^{-1}(m)}{\\sum_{j=1}^m q_j h\\circ \\beta_j^{-1}(m)}\n",
    "\\end{equation}\n",
    "\n",
    "Each of these maps $\\beta$ are defined by a scaling and shift.\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_k(m) = \\lambda_k m + b_k\n",
    "\\end{equation}\n",
    "\n",
    "There is the additional constraint on $\\lambda_k$ that they must be positive to ensure the transport is optimal (gradient of a convex function). To avaoid re-computing likelihoods, we will fix the sample values of $S(\\beta_k(m)) = L\\circ \\beta_k(m) h\\circ \\beta_k(m)$ so altering the maps changes the associated locations in the reference normal rather than in the posterior. How the position changes is therefore dependent on the inverse mapping. Letting $\\beta_k(m) = m'$, \n",
    "\n",
    "\\begin{align}\n",
    "m &= \\frac{m' - b_k}{\\lambda_k} \\\\\n",
    "\\frac{\\partial m}{\\partial b_k} &= -\\frac{1}{\\lambda_k} \\\\\n",
    "\\frac{\\partial m}{\\partial \\lambda_k} &= \\frac{b - m'}{\\lambda_k^2}\n",
    "\\end{align}\n",
    "\n",
    "Now to consider the affect of the classifier. This should alter both the values of the samples but not the positions for this case as we are not yet introducing rotations (only in 1D at the moment).\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C_k}{\\partial q_j} = -\\left(\\frac{h\\circ\\beta_j^{-1}(m)}{\\sum_{l=1}^m q_l h\\circ \\beta_l^{-1}(m)}\\right)^2 = -C_j(m)^2\n",
    "\\end{equation}\n",
    "\n",
    "It is important to remember here that, by nature of the maps,\n",
    "\n",
    "\\begin{equation}\n",
    "h\\circ\\beta_k^{-1}(m) = \\phi(m;b_k,\\lambda_k^2)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: First test for 2D is to show that the minimum summed Wasserstein along the model axes is more that when using the known principle axes. If this doesn't work, need to rethink theory...\n",
    "\n",
    "NOTE: how are we going to work out the marginal distributions from un-evenly sampled posterior? \n",
    "\n",
    "NOTE: 1D solution is dependent on CDFs so might be able to deal with projections without having to use radial basis functions or similar - think of it as mass of regions rather than mass are particular points. See diagram in Peyre and Cuturi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
